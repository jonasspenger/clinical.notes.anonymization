{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation: \n",
    "1. install conda (python 3)\n",
    "2. conda install -c conda-forge pyspark \n",
    "3. conda install -c akode jupyter-spark \n",
    "4. conda install -c conda-forge findspark \n",
    "5. conda install -c anaconda nltk \n",
    "6. conda install -c conda-forge matplotlib\n",
    "7. Download data\n",
    "6. choose settings in first block for MIMIC / Charite\n",
    "7. start and execute notebook: `jupyter notebook`, or `jupyter nbconvert --ExecutePreprocessor.timeout=0 --to notebook --execute k-anon-venn.ipynb`\n",
    "\n",
    "## Access notebook:\n",
    "* http://localhost:8888/notebooks\n",
    "\n",
    "## Access spark server:\n",
    "* http://localhost:4040/jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "\n",
    "## MIMIC Local\n",
    "spark_dir = \"/usr/local/Cellar/apache-spark/2.4.1/libexec/\"\n",
    "temp_dir = \"/Volumes/Work/TMP\"\n",
    "# temp_dir = None\n",
    "lang = \"english\"\n",
    "f_name = '../data/raw/NOTEEVENTS.csv'\n",
    "text_col = \"TEXT\"\n",
    "id_col = \"SUBJECT_ID\"\n",
    "limit = 100000  # 10'000~100 MB, 100'000~1GB~10hours\n",
    "results_df_file = \"/Users/jspenger/dev/SHK/clinicaltextnormalization/notebooks/results_df08082019.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.ml\n",
    "import pandas\n",
    "import nltk\n",
    "import numpy\n",
    "import re\n",
    "import findspark\n",
    "import pyspark.mllib.linalg.distributed\n",
    "import itertools\n",
    "import sklearn.cluster\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.dpi'] = 300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(spark_dir)\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local[*]\")\n",
    "spark = spark.config(\"spark.driver.memory\", \"8g\")\n",
    "spark = spark.config(\"spark.executor.memory\", \"8g\")\n",
    "spark = spark.config(\"spark.driver.maxResultSize\", \"8g\")\n",
    "if temp_dir is not None:\n",
    "    spark = spark.config(\"spark.local.dir\", temp_dir)\n",
    "spark = spark.getOrCreate()\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __remove_empty_tokens(l_s):\n",
    "    return list(filter(None, l_s))\n",
    "\n",
    "rm_empty_tokens = pyspark.sql.functions.udf(__remove_empty_tokens, pyspark.sql.types.ArrayType(pyspark.sql.types.StringType()))\n",
    "                \n",
    "def __sentence_tokenizer(s):\n",
    "    return nltk.tokenize.sent_tokenize(s)\n",
    "\n",
    "sentence_tokenizer = pyspark.sql.functions.udf(__sentence_tokenizer, pyspark.sql.types.ArrayType(pyspark.sql.types.StringType()))\n",
    "\n",
    "def __preprocessor(l_s):\n",
    "    return [re.sub(r'([^a-zA-Z0-9\\s]+)([a-zA-Z0-9])|([a-zA-Z0-9])([^a-zA-Z0-9\\s]+)',r'\\2\\3',s).lower() for s in l_s]\n",
    "\n",
    "preprocessor = pyspark.sql.functions.udf(__preprocessor, pyspark.sql.types.ArrayType(pyspark.sql.types.StringType()))\n",
    "\n",
    "def __non_alpha_replace(l_s):\n",
    "    return [re.sub(r'[^\\w\\s#]+', '@', re.sub('[0-9][0-9]*', '#', s)) for s in l_s]\n",
    "\n",
    "non_alpha_replace = pyspark.sql.functions.udf(__non_alpha_replace, pyspark.sql.types.ArrayType(pyspark.sql.types.StringType()))\n",
    "\n",
    "if lang == \"english\":\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "elif lang == \"german\":\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer(\"german\")\n",
    "\n",
    "def __lemmatizer(l_s):\n",
    "    return [stemmer.stem(s) for s in l_s]\n",
    "\n",
    "lemmatizer = pyspark.sql.functions.udf(__lemmatizer, pyspark.sql.types.ArrayType(pyspark.sql.types.StringType()))\n",
    "\n",
    "def __ngrams_generator(l_l_s, n):\n",
    "    return [[\" \".join(nls) for nls in zip(*[l_s[i:] for i in range(n)])] for l_s in l_l_s]\n",
    "\n",
    "ngrams_generator = pyspark.sql.functions.udf(__ngrams_generator, pyspark.sql.types.ArrayType(pyspark.sql.types.ArrayType(pyspark.sql.types.StringType())))\n",
    "\n",
    "def __nbag_from_ngram(s):\n",
    "    return \" \".join(sorted(s.split(\" \")))\n",
    "\n",
    "nbag_from_ngram = pyspark.sql.functions.udf(__nbag_from_ngram, pyspark.sql.types.StringType())\n",
    "\n",
    "def __flatten(l_l_s):\n",
    "    return [s for l_s in l_l_s for s in l_s]\n",
    "\n",
    "flatten = pyspark.sql.functions.udf(__flatten, pyspark.sql.types.ArrayType(pyspark.sql.types.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset=None, dataset_inputCol=None, dataset_outputCol=None):\n",
    "    ## train model on text sentences\n",
    "    model = pyspark.ml.feature.Word2Vec(inputCol=dataset_inputCol, outputCol=\"EMBEDDING\").fit(dataset)\n",
    "    w2v = model.getVectors().toPandas()\n",
    "    w2vv = numpy.array(list(w2v['vector'].map(lambda x: x.values)))\n",
    "    w2vw = list(w2v['word'])\n",
    "\n",
    "    ## cluster the 1 embeddings to find similar words using DBSCAN\n",
    "    dbscan = sklearn.cluster.DBSCAN(metric='cosine', eps=0.1, min_samples=5)\n",
    "    dbscan.fit(w2vv)\n",
    "\n",
    "    ## build dictionary for normalizing key to value:\n",
    "    ## key is element in cluster and value is cluster core\n",
    "    lexicon = {}\n",
    "    for i, label in enumerate(dbscan.labels_):\n",
    "        if label != -1:\n",
    "            lexicon[w2vw[i]] = w2vw[dbscan.core_sample_indices_[label]]\n",
    "\n",
    "    ## remove all keys that are also values\n",
    "    for key in set(lexicon.values()):\n",
    "        lexicon.pop(key)\n",
    "\n",
    "    if (len(lexicon) == 0):\n",
    "        return dataset.withColumn(dataset_outputCol, pyspark.sql.functions.col(dataset_inputCol))\n",
    "    else:\n",
    "        return dataset.withColumn(dataset_outputCol, \n",
    "            pyspark.sql.functions.udf(\n",
    "                lambda l_s: [lexicon.get(s, s) for s in l_s], \n",
    "                pyspark.sql.types.ArrayType(pyspark.sql.types.StringType()))(dataset_inputCol)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "results_df = None\n",
    "\n",
    "## load data\n",
    "df = spark.read.options(\n",
    "    mode='FAILFAST', \n",
    "    multiLine=True, \n",
    "    escape='\"').csv(f_name, header=True)\n",
    "\n",
    "if limit is not None:\n",
    "    df = df.limit(limit)\n",
    "\n",
    "# tokenize sentences\n",
    "df_1 = df.withColumn(\n",
    "    \"df_1\", \n",
    "    pyspark.sql.functions.explode(sentence_tokenizer(text_col)))\n",
    "# tokenize words\n",
    "df_2_old = pyspark.ml.feature.Tokenizer(\n",
    "    inputCol = \"df_1\", \n",
    "    outputCol = \"df_2_old\"\n",
    "    ).transform(df_1)\n",
    "# remove empty tokens\n",
    "df_2 = df_2_old.withColumn(\n",
    "    \"df_2\",\n",
    "    rm_empty_tokens(\"df_2_old\")).select(id_col, \"df_2\").persist(pyspark.StorageLevel.OFF_HEAP)\n",
    "\n",
    "for preprocess_bool in [True, False]:\n",
    "    if preprocess_bool:\n",
    "        df_3 = df_2.withColumn(\n",
    "            \"df_3\", \n",
    "            preprocessor(\"df_2\")).drop(\"df_2\")\n",
    "    else:\n",
    "        df_3 = df_2.withColumn(\n",
    "            \"df_3\", \n",
    "            pyspark.sql.functions.col(\"df_2\")).drop(\"df_2\")\n",
    "        \n",
    "    for non_alpha_replace_bool in [True, False]:\n",
    "        if not preprocess_bool and non_alpha_replace_bool:\n",
    "            continue\n",
    "        if non_alpha_replace_bool:\n",
    "            df_4 = df_3.withColumn(\n",
    "                \"df_4\", \n",
    "                non_alpha_replace(\"df_3\")).drop(\"df_3\")\n",
    "        else:\n",
    "            df_4 = df_3.withColumn(\"df_4\", pyspark.sql.functions.col(\"df_3\")).drop(\"df_3\")\n",
    "       \n",
    "        for lemmatize_bool in [True, False]:\n",
    "            if not non_alpha_replace_bool and lemmatize_bool:\n",
    "                continue\n",
    "            if lemmatize_bool:\n",
    "                df_5 = df_4.withColumn(\n",
    "                    \"df_5\", \n",
    "                    lemmatizer(\"df_4\")).drop(\"df_4\")\n",
    "            else:\n",
    "                df_5 = df_4.withColumn(\n",
    "                    \"df_5\", \n",
    "                    pyspark.sql.functions.col(\"df_4\")).drop(\"df_4\")\n",
    "            for normalization_bool in [True, False]:\n",
    "                if not lemmatize_bool and normalization_bool:\n",
    "                    continue\n",
    "                if normalization_bool:\n",
    "                    df_6 = normalize(\n",
    "                        dataset=df_5, \n",
    "                        dataset_inputCol=\"df_5\", \n",
    "                        dataset_outputCol=\"df_6\").drop(\"df_5\").persist(pyspark.StorageLevel.OFF_HEAP)\n",
    "                else:\n",
    "                    df_6 = df_5.withColumn(\n",
    "                        \"df_6\", \n",
    "                        pyspark.sql.functions.col(\"df_5\")).drop(\"df_5\").persist(pyspark.StorageLevel.OFF_HEAP)\n",
    "                \n",
    "                for n in [1, 2, 4, 8]:\n",
    "                    df_7 = pyspark.ml.feature.NGram(\n",
    "                        n=n, \n",
    "                        inputCol=\"df_6\", \n",
    "                        outputCol=\"NGRAMS\"\n",
    "                        ).transform(df_6)\\\n",
    "                        .withColumn(\n",
    "                            \"df_7\", \n",
    "                            pyspark.sql.functions.explode(\"NGRAMS\")).drop(\"df_6\")\n",
    "\n",
    "                    for ngram_bool in [True, False]:\n",
    "                        if not ngram_bool and n == 1:\n",
    "                            continue\n",
    "                        if not normalization_bool and not ngram_bool:\n",
    "                            continue\n",
    "                        \n",
    "                        strategy = \"\"\n",
    "                        if preprocess_bool:\n",
    "                            strategy += \" preprocess \"\n",
    "                        if non_alpha_replace_bool:\n",
    "                            strategy += \" non_alpha_replace \"\n",
    "                        if lemmatize_bool:\n",
    "                            strategy += \" lemmatize \"\n",
    "                        if normalization_bool:\n",
    "                            strategy += \" normalization \"\n",
    "\n",
    "                        if ngram_bool:\n",
    "                            strategy += \" \" + str(n) + \"-gram \"\n",
    "                        else:\n",
    "                            strategy += \" \" + str(n) + \"-bag \"\n",
    "                        if ngram_bool:\n",
    "                            df_8 = df_7.withColumn(\n",
    "                                    \"df_8\", \n",
    "                                    pyspark.sql.functions.col(\"df_7\")).drop(\"df_7\")\n",
    "                        else:\n",
    "                            df_8 = df_7.withColumn(\"df_8\", nbag_from_ngram(\"df_7\")).drop(\"df_7\")\n",
    "\n",
    "                        # count\n",
    "                        df_9 = df_8.groupBy(\"df_8\").agg(\n",
    "                            pyspark.sql.functions.count(\"df_8\").alias(\"COUNT\"), \n",
    "                            pyspark.sql.functions.countDistinct(\"df_8\", id_col).alias(\"DISTINCTCOUNT\")\n",
    "                        ).drop('df_8')\n",
    "                        \n",
    "                        result_df = df_9.withColumn(\"STRATEGY\", pyspark.sql.functions.lit(strategy))\n",
    "                        if results_df == None:\n",
    "                            results_df = result_df\n",
    "                        else:\n",
    "                            results_df = results_df.union(result_df)\n",
    "\n",
    "results_df.repartition(\"STRATEGY\").write.partitionBy(\"STRATEGY\").format(\"parquet\").save(results_df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_df = spark.read.parquet(results_df_file)\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupBy(\"STRATEGY\").agg(\n",
    "    pyspark.sql.functions.count(\"*\"),\n",
    "    pyspark.sql.functions.sum(\"COUNT\"),\n",
    "    pyspark.sql.functions.sum(\"DISTINCTCOUNT\"),\n",
    "    ).withColumn(\"order\", pyspark.sql.functions.regexp_extract(\"STRATEGY\", \"(\\d+)\" , 1))\\\n",
    "    .sort([\"order\", \"STRATEGY\"])\\\n",
    "    .show(100, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_df.select('STRATEGY').distinct().sort(\"STRATEGY\").show(1000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "strategies_df = results_df.select('STRATEGY').distinct().sort(\"STRATEGY\")\n",
    "strategies_set = [strategies_df.filter(\"STRATEGY LIKE '%{}%'\".format(i)).rdd.flatMap(lambda x: x).collect() for i in [1, 2, 4, 8]]\n",
    "print(list(strategies_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate labels\n",
    "trans_labels = {\n",
    "    '1-gram': '1g',\n",
    "    '2-gram': '2g',\n",
    "    '4-gram': '4g',\n",
    "    '8-gram': '8g',\n",
    "    '1-bag': '1gb',\n",
    "    '2-bag': '2gb',\n",
    "    '4-bag': '4gb',\n",
    "    '8-bag': '8gb',\n",
    "    'preprocess': 'pr',\n",
    "    'non_alpha_replace': 'su',\n",
    "    'lemmatize': 'le',\n",
    "    'normalization': 'wn'}\n",
    "trans_labels_inv = {v: k for k, v in trans_labels.items()}\n",
    "\n",
    "strategies_set_mod = [[\" \".join(map(lambda x: trans_labels.get(x, x), s.split())) for s in strategies] for strategies in strategies_set ]\n",
    "[l.sort() for l in strategies_set_mod]\n",
    "# strategies_set\n",
    "strategies_set = [[\" \" + \"  \".join(map(lambda x: trans_labels_inv.get(x, x), s.split())) + \" \" for s in strategies] for strategies in strategies_set_mod ]\n",
    "strategies_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# k-corpus-size\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(1.618 * 10.0 / 2.3, 10.0 / 2.0), sharex=True, sharey=True)\n",
    "axs = numpy.ravel(axs)\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bins = [1, 20, 40, 60, 80, 100, float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=bins, inputCol=\"DISTINCTCOUNT\", outputCol=\"buckets\")\n",
    "\n",
    "for i, strategies in enumerate(strategies_set):\n",
    "    \n",
    "    bar_width = 20.0/8.0\n",
    "    labels = [\" \".join(map(lambda x: trans_labels.get(x, x), s.split())) for s in strategies]\n",
    "    xticks = [i + float(bar_width) * float(len(labels)) / 2.0 for i in bins[:-1]]\n",
    "    xticklabels = [str(x) for x in bins[:-1]]\n",
    "    offsets = [i*bar_width for i in range(len(labels))]\n",
    "    \n",
    "    for offset, label, strategy in zip(offsets, labels, strategies):\n",
    "        r = results_df.filter(results_df.STRATEGY == strategy)\n",
    "        bucketData = bucketizer.transform(r)\n",
    "        data = bucketData.groupby(\"buckets\", \"STRATEGY\").agg(\n",
    "            pyspark.sql.functions.sum(\"COUNT\").alias(\"COUNTSUM\"), \n",
    "            pyspark.sql.functions.sum(\"DISTINCTCOUNT\").alias(\"DISTINCTCOUNTSUM\"),\n",
    "            pyspark.sql.functions.count(\"*\").alias(\"NUMCOUNT\")\n",
    "        ).toPandas().sort_values(by=\"buckets\")\n",
    "        hist = data[\"NUMCOUNT\"].reset_index(drop=True)\n",
    "        axs[i].bar(numpy.array(bins[:-1]) + offset, numpy.flip(numpy.cumsum(numpy.flip(hist[:]))), width=bar_width, label=label, log=True, align='edge')\n",
    "    axs[i].legend(bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", mode=\"expand\", borderaxespad=0, ncol=2)\n",
    "    axs[i].set_xticks(xticks)\n",
    "    axs[i].set_xticklabels(xticklabels)\n",
    "\n",
    "axs[0].set_ylabel(\"corpus size\")\n",
    "axs[2].set_ylabel(\"corpus size\")\n",
    "axs[2].set_xlabel(\"k\") \n",
    "axs[3].set_xlabel(\"k\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.79)\n",
    "plt.suptitle('k-anonymous corpus size')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-corpus-cover\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(1.618 * 10.0 / 2.3, 10.0 / 2.0), sharey=True)\n",
    "axs = numpy.ravel(axs)\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bins = [1, 20, 40, 60, 80, 100, float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=bins, inputCol=\"DISTINCTCOUNT\", outputCol=\"buckets\")\n",
    "\n",
    "for i, strategies in enumerate(strategies_set):\n",
    "    \n",
    "    bar_width = 20.0/8.0\n",
    "    labels = [\" \".join(map(lambda x: trans_labels.get(x, x), s.split())) for s in strategies]\n",
    "    xticks = [i + float(bar_width) * float(len(labels)) / 2.0 for i in bins[:-1]]\n",
    "    xticklabels = [str(x) for x in bins[:-1]]\n",
    "    offsets = [i*bar_width for i in range(len(labels))]\n",
    "    \n",
    "    for offset, label, strategy in zip(offsets, labels, strategies):\n",
    "        r = results_df.filter(results_df.STRATEGY == strategy)\n",
    "        bucketData = bucketizer.transform(r)\n",
    "        data = bucketData.groupby(\"buckets\", \"STRATEGY\").agg(\n",
    "            pyspark.sql.functions.sum(\"COUNT\").alias(\"COUNTSUM\"), \n",
    "            pyspark.sql.functions.sum(\"DISTINCTCOUNT\").alias(\"DISTINCTCOUNTSUM\"),\n",
    "            pyspark.sql.functions.count(\"*\").alias(\"NUMCOUNT\")\n",
    "        ).toPandas().sort_values(by=\"buckets\")\n",
    "        hist = data[\"COUNTSUM\"].reset_index(drop=True)\n",
    "        axs[i].bar(numpy.array(bins[:-1]) + offset, numpy.flip(numpy.cumsum(numpy.flip(hist[:]))) / numpy.sum(hist), width=bar_width, label=label, align='edge')\n",
    "    axs[i].legend(bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", mode=\"expand\", borderaxespad=0, ncol=2)\n",
    "    axs[i].set_xticks(xticks)\n",
    "    axs[i].set_xticklabels(xticklabels)\n",
    "#     axs[i].set_yticklabels(['{:.0%}'.format(x) for x in axs[i].get_yticks()])\n",
    "    axs[i].yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1))\n",
    "    \n",
    "axs[0].set_ylabel(\"corpus cover\")\n",
    "axs[2].set_ylabel(\"corpus cover\")\n",
    "axs[2].set_xlabel(\"k\") \n",
    "axs[3].set_xlabel(\"k\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.79)\n",
    "plt.suptitle('k-anonymous corpus cover')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-corpus-ratio\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(1.618 * 10.0 / 2.3, 10.0 / 2.0), sharey=True, sharex=True)\n",
    "axs = numpy.ravel(axs)\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bins = [1, 20, 40, 60, 80, 100, float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=bins, inputCol=\"DISTINCTCOUNT\", outputCol=\"buckets\")\n",
    "\n",
    "for i, strategies in enumerate(strategies_set):\n",
    "    \n",
    "    bar_width = 20.0/8.0\n",
    "    labels = [\" \".join(map(lambda x: trans_labels.get(x, x), s.split())) for s in strategies]\n",
    "    xticks = [i + float(bar_width) * float(len(labels)) / 2.0 for i in bins[:-1]]\n",
    "    xticklabels = [str(x) for x in bins[:-1]]\n",
    "    offsets = [i*bar_width for i in range(len(labels))]\n",
    "    \n",
    "    basic = None\n",
    "    \n",
    "    for offset, label, strategy in zip(offsets, labels, strategies):\n",
    "        r = results_df.filter(results_df.STRATEGY == strategy)\n",
    "        bucketData = bucketizer.transform(r)\n",
    "        data = bucketData.groupby(\"buckets\", \"STRATEGY\").agg(\n",
    "            pyspark.sql.functions.sum(\"COUNT\").alias(\"COUNTSUM\"), \n",
    "            pyspark.sql.functions.sum(\"DISTINCTCOUNT\").alias(\"DISTINCTCOUNTSUM\"),\n",
    "            pyspark.sql.functions.count(\"*\").alias(\"NUMCOUNT\")\n",
    "        ).toPandas().sort_values(by=\"buckets\")\n",
    "        hist = data[\"COUNTSUM\"].reset_index(drop=True)\n",
    "        if basic is None:\n",
    "            basic = numpy.flip(numpy.cumsum(numpy.flip(hist[:])))\n",
    "        x = numpy.array(bins[:-1]) + offset\n",
    "        y = numpy.flip(numpy.cumsum(numpy.flip(hist[:]))) / basic\n",
    "        axs[i].bar(x, y, width=bar_width, label=label, align='edge')\n",
    "    axs[i].legend(bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", mode=\"expand\", borderaxespad=0, ncol=2)\n",
    "    axs[i].set_xticks(xticks)\n",
    "    axs[i].set_xticklabels(xticklabels)\n",
    "    \n",
    "axs[0].set_ylabel(\"corpus ratio\")\n",
    "axs[2].set_ylabel(\"corpus ratio\")\n",
    "axs[2].set_xlabel(\"k\") \n",
    "axs[3].set_xlabel(\"k\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.79)\n",
    "plt.suptitle('k-anonymous corpus ratio')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-token-ratio\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(1.618 * 10.0 / 2.3, 10.0 / 2.0), sharex=True)\n",
    "axs = numpy.ravel(axs)\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bins = [1, 20, 40, 60, 80, 100, float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=bins, inputCol=\"COUNT\", outputCol=\"buckets\")\n",
    "\n",
    "for i, strategies in enumerate(strategies_set):\n",
    "    \n",
    "    bar_width = 20.0/8.0\n",
    "    labels = [\" \".join(map(lambda x: trans_labels.get(x, x), s.split())) for s in strategies]\n",
    "    xticks = [i + float(bar_width) * float(len(labels)) / 2.0 for i in bins[:-1]]\n",
    "    xticklabels = [str(x) for x in bins[:-1]]\n",
    "    offsets = [i*bar_width for i in range(len(labels))]\n",
    "    \n",
    "    basic = None\n",
    "    b = None\n",
    "    for offset, label, strategy in zip(offsets, labels, strategies):\n",
    "        r = results_df.filter(results_df.STRATEGY == strategy)\n",
    "        bucketData = bucketizer.transform(r)\n",
    "        data = bucketData.groupby(\"buckets\", \"STRATEGY\").agg(\n",
    "            pyspark.sql.functions.sum(\"COUNT\").alias(\"COUNTSUM\"), \n",
    "            pyspark.sql.functions.sum(\"DISTINCTCOUNT\").alias(\"DISTINCTCOUNTSUM\"),\n",
    "            pyspark.sql.functions.count(\"*\").alias(\"NUMCOUNT\")\n",
    "        ).toPandas().sort_values(by=\"buckets\")\n",
    "        hist = data[\"NUMCOUNT\"].reset_index(drop=True)\n",
    "        if basic is None:\n",
    "            basic = numpy.flip(numpy.cumsum(numpy.flip(hist[:])))\n",
    "            b = hist\n",
    "        x = numpy.array(bins[:-1]) + offset\n",
    "        y1 = numpy.flip(numpy.cumsum(numpy.flip(hist[:])))\n",
    "        acc = 0\n",
    "        accl = [0]\n",
    "        for j in range(len(basic) - 1):\n",
    "            acc = acc + b[j]\n",
    "            acc = acc - hist[j]\n",
    "            accl.append(acc)\n",
    "        y2 = basic + accl\n",
    "        y = y2 / y1\n",
    "        axs[i].bar(x, y, width=bar_width, label=label, align='edge')\n",
    "    axs[i].legend(bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\", mode=\"expand\", borderaxespad=0, ncol=2)\n",
    "    axs[i].set_xticks(xticks)\n",
    "    axs[i].set_xticklabels(xticklabels)\n",
    "    \n",
    "axs[0].set_ylabel(\"token ratio\")\n",
    "axs[2].set_ylabel(\"token ratio\")\n",
    "axs[2].set_xlabel(\"k\") \n",
    "axs[3].set_xlabel(\"k\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.79)\n",
    "plt.suptitle('k-anonymous token ratio')\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
